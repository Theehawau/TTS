{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import sys , re\n",
    "import unicodedata\n",
    "import os.path as op\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy.io import wavfile\n",
    "import pandas as pd\n",
    "import pyarabic.araby as araby\n",
    "\n",
    "import numpy as np\n",
    "import wave\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio_path', 'arabic_text', 'buckwalter_text', 'audio'],\n",
       "        num_rows: 2236\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio_path', 'arabic_text', 'buckwalter_text', 'audio'],\n",
       "        num_rows: 195\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_from_disk('/l/users/speech_lab/QASR_TTS/QASRTTS_HF')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/l/users/speech_lab/QASR_TTS/Khadija_wav/wavs/432F62C3-D353-463B-8138-98733286F1D4_48.wav',\n",
       " '')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['validation'][77]['audio_path'], df['validation'][77]['arabic_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('Khadija_wav/wavs/1D195228-25FD-4006-8702-AA2059345B26_59.wav')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root  = '/l/users/speech_lab/QASR_TTS'\n",
    "Path(df['train'][0]['audio_path']).relative_to(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_numbers = {'0': '٠', '1': '١', '2': '٢', '3': '٣', '4': '٤', '5': '٥', '6': '٦', '7': '٧', '8': '٨', '9': '٩'}\n",
    "map_numbers = dict((v, k) for k, v in map_numbers.items())\n",
    "punctuations = ''.join([chr(i) for i in list(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))])\n",
    "punctuations = punctuations + '÷#ݣ+=|$×⁄<>`åûݘ ڢ̇ پ'\n",
    "def remove_punctuation(word):\n",
    "    return word.translate(str.maketrans('', '', re.sub('[@% ]','', punctuations))).lower()\n",
    "\n",
    "def remove_diacritics(word):\n",
    "    return araby.strip_diacritics(word)\n",
    "\n",
    "def convert_numerals_to_digit(word):\n",
    "    sentence=[]\n",
    "    for w in word:\n",
    "        sentence.append(map_numbers.get(w, w))\n",
    "    word = ''.join(sentence)\n",
    "    return word\n",
    "\n",
    "def preprocess_arabic_text_v1(text):\n",
    "    # text = preprocess_arabic_twitter_text(text)\n",
    "    text = remove_diacritics(text)\n",
    "    text = convert_numerals_to_digit(text)\n",
    "    text = remove_punctuation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2236, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['train'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write(df['validation'][128]['audio_path'],df['validation'][128]['audio'],22050 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing valid split:   0%|          | 0/195 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/l/users/speech_lab/QASR_TTS/Khadija_wav/wavs/7DB7600C-D931-49BE-90FA-DE5C241B18D8_61.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     sr, wav \u001b[39m=\u001b[39m wavfile\u001b[39m.\u001b[39;49mread(row[\u001b[39m'\u001b[39;49m\u001b[39maudio_path\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     23\u001b[0m     \u001b[39massert\u001b[39;00m sr \u001b[39m==\u001b[39m \u001b[39m16000\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msampling rate \u001b[39m\u001b[39m{\u001b[39;00msr\u001b[39m}\u001b[39;00m\u001b[39m != 16000\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m     n_frames \u001b[39m=\u001b[39m wav\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/fseq/lib/python3.8/site-packages/scipy/io/wavfile.py:647\u001b[0m, in \u001b[0;36mread\u001b[0;34m(filename, mmap)\u001b[0m\n\u001b[1;32m    645\u001b[0m     mmap \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 647\u001b[0m     fid \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    649\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     file_size, is_big_endian \u001b[39m=\u001b[39m _read_riff_chunk(fid)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/l/users/speech_lab/QASR_TTS/Khadija_wav/wavs/7DB7600C-D931-49BE-90FA-DE5C241B18D8_61.wav'"
     ]
    }
   ],
   "source": [
    "# splits = {'train':'train', 'validation':'valid'}\n",
    "splits = {'validation':'valid'}\n",
    "female = \"spk_emb/qasr_KhadijaBenguenna_embedding.npy\"\n",
    "male = \"spk_emb/qasr_MahmoudMourad_embedding.npy\"\n",
    "\n",
    "for key, split in splits.items():\n",
    "    save_tsv = f\"/l/users/speech_lab/_SpeechT5PretrainDataset/Finetune/TTS/hubert_labels/QASRTTS/{split}.tsv\"\n",
    "    save_txt = f\"/l/users/speech_lab/_SpeechT5PretrainDataset/Finetune/TTS/labels/QASRTTS/{split}.txt\"\n",
    "    tsv = open(save_tsv, 'w')\n",
    "    txt = open(save_txt, 'w')\n",
    "    print(f\"{root}\", file=tsv)\n",
    "    for row in tqdm(df[key], total=df[key].shape[0], desc=f\"Processing {split} split\"):\n",
    "        # try:\n",
    "            # assert Path(row['audio_path']).exists(), f\"{row['audio_path']} does not exist\"\n",
    "            # print(f\"audio: {row['audio_path']}\")\n",
    "            # wav,sr = librosa.load(row['audio_path'])\n",
    "            # wav = librosa.resample(wav, sr, 16000)\n",
    "            # wavfile.write(row['audio_path'], 16000, wav)\n",
    "        if row['audio_path'] == \"Khadija_wav/wavs/432F62C3-D353-463B-8138-98733286F1D4_48.wav\":\n",
    "            continue\n",
    "        else:\n",
    "            sr, wav = wavfile.read(row['audio_path'])\n",
    "            assert sr == 16000, f\"sampling rate {sr} != 16000\"\n",
    "            n_frames = wav.shape[0]\n",
    "            audio_path = Path(row['audio_path']).relative_to(root)\n",
    "            spk_emb = female if Path(row['audio_path']).parts[5] == 'Khadija_wav' else male\n",
    "            print(f\"{audio_path}\\t{n_frames}\\t{spk_emb}\", file=tsv)\n",
    "            print(f\"{preprocess_arabic_text_v1(row['arabic_text'])}\", file=txt)\n",
    "        # except AssertionError:\n",
    "        #     # wavfile.write(np.array(row['audio_path']), 22050, row['audio'])\n",
    "        #     # wav,sr = librosa.load(row['audio_path'])\n",
    "        #     print(f\"Resampling {row['audio_path']}\")\n",
    "        #     wav = librosa.resample(np.array(row['audio']), 22050, 16000)\n",
    "        #     sf.write(row['audio_path'],wav, sr=16000)\n",
    "        #     wav = wave.open(row['audio_path'], 'r')\n",
    "        #     n_frames = wav.getnframes()\n",
    "        #     # sr, wav = wavfile.read(row['audio_path'])\n",
    "        #     # n_frames = wav.shape[0]\n",
    "        #     audio_path = Path(row['audio_path']).relative_to(root)\n",
    "        #     spk_emb = female if Path(row['audio_path']).parts[5] == 'Khadija_wav' else male\n",
    "        #     print(f\"{audio_path}\\t{n_frames}\\t{spk_emb}\", file=tsv)\n",
    "        #     print(f\"{remove_punctuation(row['arabic_text'])}\", file=txt)\n",
    "    tsv.close()\n",
    "    txt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fseq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
